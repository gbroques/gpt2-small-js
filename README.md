# gpt2-small-js

JavaScript implementation of Generative Pre-trained Transformer 2 ([GPT-2](https://en.wikipedia.org/wiki/GPT-2)) small (124 million parameters) based on Ishan Anand's [spreadsheets are all you need.ai](https://spreadsheets-are-all-you-need.ai/gpt2/) implementation for educational purposes.

## Why study GPT-2?

GPT-2 was introduced by [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) in 2019.

It is the predecessor of state-of-the-art large language models (LLMs) such as ChatGPT (2022), Claude, Llama, and Bard.

The following diagram shows this relationship:

[![LLM family tree](./llm-family-tree.jpg)](https://github.com/Mooler0410/LLMsPracticalGuide)

GPT-2 is based on the transformer architecture introduced by [Attention Is All You Need](https://en.wikipedia.org/wiki/Attention_Is_All_You_Need) in 2017, and shares similar architecture to the wildly successful ChatGPT model.

The main differences between GPT-2 and ChatGPT are scale (in terms of both parameter count and training dataset), and how they were trained.

Anand claims understanding GPT-2 allows understanding 80% of state-of-the-art models.

## Simplified Architecture

GPT-2 takes in text as input and outputs the most-likely next token using the following simplified architecture:

1. [Tokenization](#tokenization)
2. [Token & Position Embeddings](#token--position-embeddings)
3. (loop 12 times)
    1. Multi-head Attention
    2. Multilayer Perceptron
4. Language Head

Understanding next token predicion enables understanding most of the process since longer text sequences can be generated by appending the output to the input and re-running the model.

The first two steps, tokenization and embeddings, are to map text to numbers for the computer to process them.

## Tokenization

Tokenization is the process of splitting text into smaller units called *tokens*.

Different tokenization algorithms output tokens with different scales such as words, subwords and characters.

The algorithm GPT-2 uses for tokenization is [byte-pair encoding (BPE)](#byte-pair-encoding-bpe) which outputs sub-word tokens.

Each token represents an item in the model's vocabulary, and receives a token **id**entifier which is the position in the dictionary.

## Why subword tokens?

Word-based tokenization increases the size of the model's vocabulary.

This leads to an increase in the number of model parameters, and hence space and time complexity.

For example, English has ~170,000 words where GPT-2 has ~50,000 tokens in its vocabulary.

Character-based tokenization also increases the space and time complexity since it increase the number of input tokens.

Each token receives an embedding.

There's also low semantic correlation between characters.

For example, you can read the following passage even though the characters are jumbled:

> Aoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn't mttaer in waht oredr the ltteers in a wrod are, the olny iprmoetnt tihng is taht the frist and lsat ltteer be at the rghit pclae.
The rset can be a toatl mses and you can sitll raed it wouthit porbelm.
Tihs is bcuseae the huamn mnid deos not raed ervey lteter by istlef, but the wrod as a wlohe.

Subword tokenization lies somewhere in between word-based and character-based tokenization.

### Byte-pair Encoding (BPE)

[Byte-pair encoding (BPE)](https://en.wikipedia.org/wiki/Byte-pair_encoding) is a compression algorithm introduced by Philip Gage in "A New Algorithm for Data Compression (1994)".

[Neural Machine Translation of Rare Words with Subword Units (2015)](https://arxiv.org/abs/1508.07909) adapts BPE for subword tokenization by using characters instead of bytes.

It consists of two phases:

1. **Learning** - Learn common subwords
    1. Input a large corpus of text
    2. Apply BPE learning algorithm
    3. Output a vocabulary, or dictionary of tokens (`vocab_bpe.csv`, `bpe_token_ids.csv`)
2. **Tokenization** - Split words into tokens
    1. Input text & vocabulary
    2. Apply BPE tokenization algorithm
    3. Output tokens

**Reference:** [Lesson 2: Byte Pair Encoding in AI Explained with a Spreadsheet](https://www.youtube.com/watch?v=PvZN3-WqAOI)

## Token & Position Embeddings

### Token Embeddings

Every token receives an embedding.

An embedding is a list of numbers that represent both the meaning and position of the token in the prompt.

In GPT-2 small, the number of tokens is 50,257 and the embedding dimension is 768.

The embedding values are learned during training.

The `model_wte.csv` file are the learned values during training known as the text embedding matrix.

It contributes 50,257 * 768 = 38,597,376 or roughly 39 million parameters to the overall 124 million parameters of the model.

Using English as the vocabulary, with 170,000 words, would nearly double the numbers of parameters of the model.

**References:**
* [Lesson 3: Understanding Word Embeddings in AI and LLMs](https://www.youtube.com/watch?v=v6yD5SOxOXI)
* [33:11 How LLMs work for Web Devs: GPT in 600 lines of Vanilla JS - Ishan Anand](https://youtu.be/ZuiJjkbX0Og?t=1991)

### Position Embeddings

In English, word order matters.

"The dog chases the cat" is different than "the cat chases the dog".

To capture position, we slightly move the token's embedding in embeddings space based on the position of the token in the prompt.

In [Attention Is All You Need](https://papers.nips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (see section "3.5 Positional Encoding"), the author's used `sin` and `cos` functions to slightly offset the token's embedding based on its position in the prompt.

In GPT-2, learned positional embeddings were instead used (see "Model specifications" in section 4.1 on page 5 of [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)).

The `model_wpe.csv` file contains the learned values during training known as the position embedding matrix.

It has a dimension of 1024 x 768 where 1024 is the model's max context size.

This matrix is simply added to the token embeddings to produce the positioned embeddings.

Modern LLMs don't follow this approach for position embeddings, and instead use [rotary position embedding (RoPE)](https://arxiv.org/abs/2104.09864).

**References:**
* [49:41 How LLMs work for Web Devs: GPT in 600 lines of Vanilla JS - Ishan Anand](https://youtu.be/ZuiJjkbX0Og?t=2981)

## References

* [How LLMs work for Web Devs: GPT in 600 lines of Vanilla JS - Ishan Anand](https://www.youtube.com/watch?v=ZuiJjkbX0Og)
* [GPT-2 Architecture Demystified: A Step-by-Step Breakdown](https://sararavi14.medium.com/gpt-2-architecture-demystified-a-step-by-step-breakdown-74b1c5c80d17)

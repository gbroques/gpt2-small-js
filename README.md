# gpt2-small-js

JavaScript implementation of Generative Pre-trained Transformer 2 ([GPT-2](https://en.wikipedia.org/wiki/GPT-2)) small (124 million parameters) based on Ishan Anand's [spreadsheets are all you need.ai](https://spreadsheets-are-all-you-need.ai/gpt2/) implementation for educational purposes.

## Why study GPT-2?

GPT-2 (2019) is the predecessor of state-of-the-art large language models (LLMs) such as ChatGPT (2022), Claude, Llama, and Bard.

The following diagram shows this relationship:

[![LLM family tree](./llm-family-tree.jpg)](https://github.com/Mooler0410/LLMsPracticalGuide)

GPT-2 is based on the transformer architecture introduced by [Attention Is All You Need](https://en.wikipedia.org/wiki/Attention_Is_All_You_Need) in 2017, and shares similar architecture to the wildly successful ChatGPT model.

The main differences between GPT-2 and ChatGPT are scale (in terms of both parameter count and training dataset), and how they were trained.

Anand claims understanding GPT-2 allows understanding 80% of state-of-the-art models.

## Simplified Architecture

GPT-2 takes in text as input and outputs the most-likely next token using the following simplified architecture:

1. Tokenization
2. Text & Position Embeddings
3. (loop 12 times)
    1. Multi-head Attention
    2. Multilayer Perceptron
4. Language Head

Understanding next token predicion enables understanding most of the process since longer text sequences can be generated by appending the output to the input and re-running the model.

## References

* [How LLMs work for Web Devs: GPT in 600 lines of Vanilla JS - Ishan Anand](https://www.youtube.com/watch?v=ZuiJjkbX0Og)

